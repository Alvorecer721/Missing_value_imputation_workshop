{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical time series missing values imputation techniques\n",
    "\n",
    "In this notebook, we explore various missing value imputation techniques for time series data using the PhysioNet Challenge 2012 dataset. The objective of this workshop is to introduce different imputation methods and demonstrate their implementations using open-source libraries.\n",
    "\n",
    "For the PhysioNet 2012 dataset, we compare several advanced imputation techniques with a naive baseline, focusing on their impact on machine learning performance. Specifically, we train a machine learning model to perform a downstream prediction task and analyze the differences in performance between the naive baseline and the versions with imputed data.\n",
    "\n",
    "The primary library used in this workshop is **PyPOTS**, a versatile framework that incorporates state-of-the-art models for time series imputation and continues to expand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-03 17:13:43.899060: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-03 17:13:47.881592: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-03 17:13:51.633371: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-03 17:13:54.590212: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-03 17:13:55.389418: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-03 17:14:00.147597: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-03 17:14:12.897098: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "!pip install pypots\n",
    "!pip install tsdb\n",
    "!pip install torch_geometric torch_scatter torch_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypots\n",
    "import tsdb\n",
    "from pypots.optim import Adam\n",
    "from pypots.classification import Raindrop\n",
    "from pypots.utils.metrics import calc_binary_classification_metrics\n",
    "from pypots.utils.random import set_random_seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_numpy_from_url(url):\n",
    "\t\"\"\"\n",
    "\tLoad a numpy array from a URL.\n",
    "\n",
    "\tParameters:\n",
    "\t\turl (str): The URL to load the numpy array from.\n",
    "\n",
    "\tReturn:\n",
    "\t\tnp.ndarray: The numpy array.\n",
    "\t\"\"\"\n",
    "\t# Fetch the data from the URL\n",
    "\tresponse = urllib.request.urlopen(url)\n",
    "\n",
    "\t# Stream the data into a numpy array\n",
    "\treturn np.load(io.BytesIO(response.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 06:48:14 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: \n",
      "https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012\n",
      "2024-09-05 06:48:14 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...\n",
      "2024-09-05 06:48:14 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 06:48:14 [INFO]: Loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "data = tsdb.load(\"physionet_2012\")\n",
    "feature = pd.concat([data[\"set-a\"], data[\"set-b\"], data[\"set-c\"]], axis=0)\n",
    "label = pd.concat([data[\"outcomes-a\"], data[\"outcomes-b\"], data[\"outcomes-c\"]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 48\n",
    "missing_threshold = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retain records that span over 48 hours and have less than 70% missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1165,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_feature = (\n",
    "    feature.groupby(\"RecordID\")\n",
    "    .filter(\n",
    "        lambda group: len(group) == n_steps  # 48 hours of data\n",
    "        and (group.isnull().sum().sum() / group.size)\n",
    "        < missing_threshold  # Missing values < missing_threshold\n",
    "    )\n",
    "    .dropna(axis=1, how=\"all\")  # Drop columns where all elements are NaN\n",
    "    .sort_values(by=[\"RecordID\", \"Time\"])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_3d_array(df_feature, df_label, groupby_col):\n",
    "    \"\"\"\n",
    "    Converts a DataFrame into a 3D NumPy array for features and a 1D NumPy array for labels.\n",
    "\n",
    "    Parameters:\n",
    "    df_feature: \tpandas.DataFrame, the feature DataFrame containing data to be converted into a 3D array.\n",
    "    df_label: \t \tpandas.Series, the label Series containing the labels, indexed by `groupby_col` values.\n",
    "    groupby_col:\tstr, the column in `df_feature` used to group the data.\n",
    "\n",
    "    Returns:\n",
    "    feature_3d: \tnp.ndarray, A 3D NumPy array where each \"slice\" (first dimension) corresponds to one group of features\n",
    "    label_1d : \t\tnp.ndarray, A 1D NumPy array containing the labels corresponding to each group in the same order\n",
    "    \"\"\"\n",
    "\n",
    "    grouped = df_feature.groupby(groupby_col)\n",
    "\n",
    "    groupby_ids, arrays = zip(\n",
    "        *[\n",
    "            (groupby_id, group.drop(columns=[\"RecordID\"]).values)\n",
    "            for groupby_id, group in grouped\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    feature_3d = np.stack(arrays)\n",
    "    label_1d = df_label.loc[list(groupby_ids)].to_numpy().flatten()\n",
    "\n",
    "    return feature_3d, label_1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5666/2807903151.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(\n"
     ]
    }
   ],
   "source": [
    "# TODO: Calculate the overall mean for each column across the entire dataset\n",
    "overall_means = ...\n",
    "\n",
    "# Group by 'RecordID' and fill NaN values with the group's mean where possible\n",
    "# For columns where all values are NaN, fill them with the overall column mean\n",
    "mean_imputed_feature = (\n",
    "    filtered_feature.groupby(\"RecordID\")\n",
    "    .apply(\n",
    "        lambda group: group.fillna(group.mean()).fillna(  # Fill with group's mean\n",
    "            overall_means\n",
    "        )\n",
    "    )  # Fill remaining NaNs (i.e., columns where all values were NaN) with overall mean\n",
    "    .reset_index(drop=True)  # Drop the 'RecordID' index\n",
    "    .sort_values(by=[\"RecordID\", \"Time\"])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_imputed, mean_label = dataframe_to_3d_array(\n",
    "    df_feature=mean_imputed_feature, df_label=label, groupby_col=\"RecordID\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Score-based Diffusion Model for Probabilistic Timer Series Imputation (CSDI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although autoregressive models are commonly used for time series imputation, score-based diffusion models have recently surpassed them in performance across various tasks, including image generation and audio synthesis. Given these advancements, it’s worth exploring their potential for time series imputation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1171,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_feature_3d, filtered_label = dataframe_to_3d_array(\n",
    "    df_feature=filtered_feature, df_label=label, groupby_col=\"RecordID\"\n",
    ")\n",
    "\n",
    "# TODO: fill in the corresponding datasets\n",
    "dataset_for_imputation = {\n",
    "    \"X\": ...,\n",
    "    \"y\": ...,\n",
    "}\n",
    "\n",
    "n_features = dataset_for_imputation[\"X\"].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 07:06:16 [INFO]: Have set the random seed as 42 for numpy and pytorch.\n",
      "2024-09-05 07:06:16 [INFO]: No given device, using default device: cpu\n",
      "2024-09-05 07:06:16 [INFO]: Model files will be saved to ./csdi/20240905_T070616\n",
      "2024-09-05 07:06:16 [INFO]: Tensorboard file will be saved to ./csdi/20240905_T070616/tensorboard\n",
      "2024-09-05 07:06:16 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 873,153\n",
      "2024-09-05 07:08:31 [INFO]: Epoch 001 - training loss: 0.8558\n",
      "2024-09-05 07:08:31 [INFO]: Saved the model to ./csdi/20240905_T070616/CSDI_epoch1_loss0.8558165751970731.pypots\n",
      "2024-09-05 07:10:53 [INFO]: Epoch 002 - training loss: 0.7794\n",
      "2024-09-05 07:10:54 [INFO]: Saved the model to ./csdi/20240905_T070616/CSDI_epoch2_loss0.7794200258377271.pypots\n",
      "2024-09-05 07:13:05 [INFO]: Epoch 003 - training loss: 0.7343\n",
      "2024-09-05 07:13:06 [INFO]: Saved the model to ./csdi/20240905_T070616/CSDI_epoch3_loss0.7342884846222706.pypots\n",
      "2024-09-05 07:15:17 [INFO]: Epoch 004 - training loss: 0.7119\n",
      "2024-09-05 07:15:17 [INFO]: Saved the model to ./csdi/20240905_T070616/CSDI_epoch4_loss0.7118911330516522.pypots\n",
      "2024-09-05 07:17:28 [INFO]: Epoch 005 - training loss: 0.7046\n",
      "2024-09-05 07:17:28 [INFO]: Saved the model to ./csdi/20240905_T070616/CSDI_epoch5_loss0.7046489929541563.pypots\n",
      "2024-09-05 07:19:42 [INFO]: Epoch 006 - training loss: 0.7018\n",
      "2024-09-05 07:19:43 [INFO]: Saved the model to ./csdi/20240905_T070616/CSDI_epoch6_loss0.701783788509858.pypots\n",
      "2024-09-05 07:22:02 [INFO]: Epoch 007 - training loss: 0.6785\n",
      "2024-09-05 07:22:02 [INFO]: Saved the model to ./csdi/20240905_T070616/CSDI_epoch7_loss0.6784520561878498.pypots\n",
      "2024-09-05 07:24:21 [INFO]: Epoch 008 - training loss: 0.6642\n",
      "2024-09-05 07:24:22 [INFO]: Saved the model to ./csdi/20240905_T070616/CSDI_epoch8_loss0.6641767468207922.pypots\n",
      "2024-09-05 07:26:39 [INFO]: Epoch 009 - training loss: 0.6643\n",
      "2024-09-05 07:28:54 [INFO]: Epoch 010 - training loss: 0.6505\n",
      "2024-09-05 07:28:54 [INFO]: Saved the model to ./csdi/20240905_T070616/CSDI_epoch10_loss0.6505030806248004.pypots\n",
      "2024-09-05 07:28:54 [INFO]: Finished training. The best model is from epoch#10.\n",
      "2024-09-05 07:28:54 [INFO]: Saved the model to ./csdi/20240905_T070616/CSDI.pypots\n"
     ]
    }
   ],
   "source": [
    "from pypots.imputation import CSDI\n",
    "\n",
    "set_random_seed(42)\n",
    "\n",
    "# initialize the model\n",
    "csdi = CSDI(\n",
    "    n_features=n_features,\n",
    "    n_steps=1,\n",
    "    n_layers=3,\n",
    "    n_heads=2,\n",
    "    n_channels=128,\n",
    "    d_time_embedding=64,\n",
    "    d_feature_embedding=32,\n",
    "    d_diffusion_embedding=128,\n",
    "    target_strategy=\"random\",\n",
    "    n_diffusion_steps=50,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")\n",
    "\n",
    "\n",
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "csdi.fit(train_set=dataset_for_imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1176,
   "metadata": {},
   "outputs": [],
   "source": [
    "csdi_imputed = csdi.predict(dataset_for_imputation)[\"imputation\"].squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bidirectional Recurrent Imputation for Time Series (BRITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 12:45:17 [INFO]: No given device, using default device: cpu\n",
      "2024-09-05 12:45:17 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-09-05 12:45:17 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 255,344\n",
      "2024-09-05 12:45:40 [INFO]: Epoch 001 - training loss: 142.2712\n",
      "2024-09-05 12:45:58 [INFO]: Epoch 002 - training loss: 108.8362\n",
      "2024-09-05 12:46:20 [INFO]: Epoch 003 - training loss: 86.8353\n",
      "2024-09-05 12:46:38 [INFO]: Epoch 004 - training loss: 77.1769\n",
      "2024-09-05 12:46:56 [INFO]: Epoch 005 - training loss: 72.4962\n",
      "2024-09-05 12:47:14 [INFO]: Epoch 006 - training loss: 69.0645\n",
      "2024-09-05 12:47:33 [INFO]: Epoch 007 - training loss: 66.2519\n",
      "2024-09-05 12:47:50 [INFO]: Epoch 008 - training loss: 63.5382\n",
      "2024-09-05 12:48:11 [INFO]: Epoch 009 - training loss: 61.2091\n",
      "2024-09-05 12:48:28 [INFO]: Epoch 010 - training loss: 59.0752\n",
      "2024-09-05 12:48:46 [INFO]: Epoch 011 - training loss: 57.2260\n",
      "2024-09-05 12:49:03 [INFO]: Epoch 012 - training loss: 55.3654\n",
      "2024-09-05 12:49:22 [INFO]: Epoch 013 - training loss: 53.7939\n",
      "2024-09-05 12:49:39 [INFO]: Epoch 014 - training loss: 52.5743\n",
      "2024-09-05 12:50:03 [INFO]: Epoch 015 - training loss: 51.1263\n",
      "2024-09-05 12:50:28 [INFO]: Epoch 016 - training loss: 50.0991\n",
      "2024-09-05 12:50:50 [INFO]: Epoch 017 - training loss: 49.3564\n",
      "2024-09-05 12:51:07 [INFO]: Epoch 018 - training loss: 48.2772\n",
      "2024-09-05 12:51:24 [INFO]: Epoch 019 - training loss: 47.5728\n",
      "2024-09-05 12:51:41 [INFO]: Epoch 020 - training loss: 46.9491\n",
      "2024-09-05 12:51:41 [INFO]: Finished training. The best model is from epoch#20.\n"
     ]
    }
   ],
   "source": [
    "from pypots.imputation import BRITS\n",
    "\n",
    "set_random_seed(42)\n",
    "\n",
    "# initialize the model\n",
    "brits = BRITS(\n",
    "    n_steps=n_steps,\n",
    "    n_features=n_features,\n",
    "    rnn_hidden_size=128,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=20,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")\n",
    "\n",
    "brits.fit(train_set=dataset_for_imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1275,
   "metadata": {},
   "outputs": [],
   "source": [
    "brits_imputed = brits.predict(dataset_for_imputation)[\"imputation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1286,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(imputed_data, label_data, test_size=0.2, val_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the imputed data and label data into training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    imputed_data: \tnp.ndarray, the imputed data to be split.\n",
    "    label_data: \tnp.ndarray, the label data to be split.\n",
    "    test_size: \t\tfloat, optional, the proportion of the dataset to include in the test split.\n",
    "    val_size: \t\tfloat, optional, the proportion of the dataset to include in the validation split.\n",
    "    random_state: \tint, optional, random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    datasets: \t\tdict, a dictionary containing the training, validation, and testing sets.\n",
    "    \"\"\"\n",
    "\n",
    "    train_val_data, test_data, train_val_label, test_label = train_test_split(\n",
    "        imputed_data,  # The 3D array of features\n",
    "        label_data,  # Corresponding labels\n",
    "        test_size=test_size,  # Test data size\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    train_data, val_data, train_label, val_label = train_test_split(\n",
    "        train_val_data,   # Remaining train+validation features\n",
    "        train_val_label,  # Remaining train+validation labels\n",
    "        test_size=val_size\n",
    "        / (\n",
    "            1 - test_size\n",
    "        ),  # Adjust validation size relative to the train+validation set\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    datasets = {\n",
    "        \"training\": {\"X\": train_data, \"y\": train_label},\n",
    "        \"validation\": {\"X\": val_data, \"y\": val_label},\n",
    "        \"testing\": {\"X\": test_data, \"y\": test_label},\n",
    "    }\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Mean Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Imputation\n",
    "datasets = split_data(mean_imputed, mean_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For time series classification, we employ a graph neural network specifically optimized to capture time-dependent dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1288,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 12:58:21 [INFO]: Have set the random seed as 2022 for numpy and pytorch.\n",
      "2024-09-05 12:58:21 [INFO]: No given device, using default device: cpu\n",
      "2024-09-05 12:58:21 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-09-05 12:58:21 [INFO]: Raindrop initialized with the given hyperparameters, the number of trainable parameters: 1,541,396\n"
     ]
    }
   ],
   "source": [
    "set_random_seed()\n",
    "\n",
    "# initialize the model\n",
    "raindrop = Raindrop(\n",
    "    n_steps=n_steps,\n",
    "    n_features=n_features,\n",
    "    n_classes=2,\n",
    "    n_layers=2,\n",
    "    d_model=n_features * 4,\n",
    "    d_ffn=256,\n",
    "    n_heads=2,\n",
    "    dropout=0.4,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=20,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=5,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=5e-4),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    model_saving_strategy=\"best\",  # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1289,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 12:58:34 [INFO]: Epoch 001 - training loss: 0.5450, validation loss: 0.4540\n",
      "2024-09-05 12:58:43 [INFO]: Epoch 002 - training loss: 0.5259, validation loss: 0.4183\n",
      "2024-09-05 12:58:53 [INFO]: Epoch 003 - training loss: 0.5061, validation loss: 0.4000\n",
      "2024-09-05 12:59:02 [INFO]: Epoch 004 - training loss: 0.4926, validation loss: 0.4027\n",
      "2024-09-05 12:59:10 [INFO]: Epoch 005 - training loss: 0.4841, validation loss: 0.4018\n",
      "2024-09-05 12:59:18 [INFO]: Epoch 006 - training loss: 0.4734, validation loss: 0.4156\n",
      "2024-09-05 12:59:28 [INFO]: Epoch 007 - training loss: 0.4934, validation loss: 0.4175\n",
      "2024-09-05 12:59:40 [INFO]: Epoch 008 - training loss: 0.4795, validation loss: 0.4288\n",
      "2024-09-05 12:59:40 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2024-09-05 12:59:40 [INFO]: Finished training. The best model is from epoch#3.\n"
     ]
    }
   ],
   "source": [
    "# Fit and predict\n",
    "raindrop.fit(train_set=datasets[\"training\"], val_set=datasets[\"validation\"])\n",
    "pred_mean = raindrop.predict(datasets[\"testing\"])[\"classification\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing classification metrics: \n",
      "ROC_AUC: 0.6620135363790186, \n",
      "Accuracy: 0.8,\n",
      "F1: 0.19672131147540983,\n",
      "Precision: 0.46153846153846156,\n",
      "Recall: 0.125,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate the values of binary classification metrics on the model's prediction\n",
    "metrics = calc_binary_classification_metrics(pred_mean, datasets[\"testing\"][\"y\"])\n",
    "print(\n",
    "    \"Testing classification metrics: \\n\"\n",
    "    f'ROC_AUC: {metrics[\"roc_auc\"]}, \\n'\n",
    "    f'Accuracy: {metrics[\"accuracy\"]},\\n'\n",
    "    f'F1: {metrics[\"f1\"]},\\n'\n",
    "    f'Precision: {metrics[\"precision\"]},\\n'\n",
    "    f'Recall: {metrics[\"recall\"]},\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating CSDI Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSDI Imputation\n",
    "\n",
    "csdi_imputed = load_numpy_from_url(url='https://raw.githubusercontent.com/Alvorecer721/Missing_value_imputation_workshop/d870767db751380283661f62cbc3439f683433ed/data/time-series/csdi_imputed.npy')\n",
    "label = load_numpy_from_url(url='https://raw.githubusercontent.com/Alvorecer721/Missing_value_imputation_workshop/d870767db751380283661f62cbc3439f683433ed/data/time-series/label.npy')\n",
    "datasets = split_data(csdi_imputed, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1252,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 11:45:12 [INFO]: Have set the random seed as 2022 for numpy and pytorch.\n",
      "2024-09-05 11:45:12 [INFO]: No given device, using default device: cpu\n",
      "2024-09-05 11:45:12 [INFO]: Model files will be saved to tutorial_results/classification/raindrop/20240905_T114512\n",
      "2024-09-05 11:45:12 [INFO]: Tensorboard file will be saved to tutorial_results/classification/raindrop/20240905_T114512/tensorboard\n",
      "2024-09-05 11:45:12 [INFO]: Raindrop initialized with the given hyperparameters, the number of trainable parameters: 1,541,396\n"
     ]
    }
   ],
   "source": [
    "set_random_seed()\n",
    "\n",
    "# initialize the model\n",
    "raindrop = Raindrop(\n",
    "    n_steps=n_steps,\n",
    "    n_features=n_features,\n",
    "    n_classes=2,\n",
    "    n_layers=2,\n",
    "    d_model=n_features * 4,\n",
    "    d_ffn=256,\n",
    "    n_heads=2,\n",
    "    dropout=0.4,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=20,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=5,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=5e-4),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    model_saving_strategy=\"best\",  # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 11:45:22 [INFO]: Epoch 001 - training loss: 0.5450, validation loss: 0.4540\n",
      "2024-09-05 11:45:22 [INFO]: Saved the model to tutorial_results/classification/raindrop/20240905_T114512/Raindrop_epoch1_loss0.4539904296398163.pypots\n",
      "2024-09-05 11:45:30 [INFO]: Epoch 002 - training loss: 0.5259, validation loss: 0.4183\n",
      "2024-09-05 11:45:30 [INFO]: Saved the model to tutorial_results/classification/raindrop/20240905_T114512/Raindrop_epoch2_loss0.41831769049167633.pypots\n",
      "2024-09-05 11:45:40 [INFO]: Epoch 003 - training loss: 0.5061, validation loss: 0.4000\n",
      "2024-09-05 11:45:40 [INFO]: Saved the model to tutorial_results/classification/raindrop/20240905_T114512/Raindrop_epoch3_loss0.39998289197683334.pypots\n",
      "2024-09-05 11:45:48 [INFO]: Epoch 004 - training loss: 0.4926, validation loss: 0.4027\n",
      "2024-09-05 11:45:56 [INFO]: Epoch 005 - training loss: 0.4841, validation loss: 0.4018\n",
      "2024-09-05 11:46:04 [INFO]: Epoch 006 - training loss: 0.4734, validation loss: 0.4156\n",
      "2024-09-05 11:46:13 [INFO]: Epoch 007 - training loss: 0.4934, validation loss: 0.4175\n",
      "2024-09-05 11:46:21 [INFO]: Epoch 008 - training loss: 0.4795, validation loss: 0.4288\n",
      "2024-09-05 11:46:21 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2024-09-05 11:46:21 [INFO]: Finished training. The best model is from epoch#3.\n",
      "2024-09-05 11:46:22 [INFO]: Saved the model to tutorial_results/classification/raindrop/20240905_T114512/Raindrop.pypots\n"
     ]
    }
   ],
   "source": [
    "# Fit amd predict\n",
    "raindrop.fit(train_set=datasets[\"training\"], val_set=datasets[\"validation\"])\n",
    "\n",
    "pred_csdi = raindrop.predict(datasets[\"testing\"])[\"classification\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing classification metrics: \n",
      "ROC_AUC: 0.7125634517766498, \n",
      "Accuracy: 0.8040816326530612,\n",
      "F1: 0.4,\n",
      "Precision: 0.5,\n",
      "Recall: 0.3333333333333333,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate the values of binary classification metrics on the model's prediction\n",
    "metrics = calc_binary_classification_metrics(pred_csdi, datasets[\"testing\"][\"y\"])\n",
    "print(\n",
    "    \"Testing classification metrics: \\n\"\n",
    "    f'ROC_AUC: {metrics[\"roc_auc\"]}, \\n'\n",
    "    f'Accuracy: {metrics[\"accuracy\"]},\\n'\n",
    "    f'F1: {metrics[\"f1\"]},\\n'\n",
    "    f'Precision: {metrics[\"precision\"]},\\n'\n",
    "    f'Recall: {metrics[\"recall\"]},\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating BRITS Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BRITS Imputation\n",
    "brits_imputed = load_numpy_from_url(url='https://raw.githubusercontent.com/Alvorecer721/Missing_value_imputation_workshop/d870767db751380283661f62cbc3439f683433ed/data/time-series/brits_imputed.npy')\n",
    "label = load_numpy_from_url(url='https://raw.githubusercontent.com/Alvorecer721/Missing_value_imputation_workshop/d870767db751380283661f62cbc3439f683433ed/data/time-series/label.npy')\n",
    "datasets = split_data(brits_imputed, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1280,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 12:53:57 [INFO]: Have set the random seed as 2022 for numpy and pytorch.\n",
      "2024-09-05 12:53:57 [INFO]: No given device, using default device: cpu\n",
      "2024-09-05 12:53:57 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "/anaconda/envs/workshop/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/anaconda/envs/workshop/lib/python3.10/site-packages/pypots/nn/modules/raindrop/backbone.py:119: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(self.R_u)  # xavier_uniform also known as glorot\n",
      "2024-09-05 12:53:57 [INFO]: Raindrop initialized with the given hyperparameters, the number of trainable parameters: 1,541,396\n"
     ]
    }
   ],
   "source": [
    "set_random_seed()\n",
    "\n",
    "# initialize the model\n",
    "raindrop = Raindrop(\n",
    "    n_steps=n_steps,\n",
    "    n_features=n_features,\n",
    "    n_classes=2,\n",
    "    n_layers=2,\n",
    "    d_model=n_features * 4,\n",
    "    d_ffn=256,\n",
    "    n_heads=2,\n",
    "    dropout=0.4,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=20,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=5,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=5e-4),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    model_saving_strategy=\"best\",  # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1281,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 12:54:25 [INFO]: Epoch 001 - training loss: 0.5352, validation loss: 0.4740\n",
      "2024-09-05 12:54:34 [INFO]: Epoch 002 - training loss: 0.5304, validation loss: 0.4389\n",
      "2024-09-05 12:54:43 [INFO]: Epoch 003 - training loss: 0.5050, validation loss: 0.4072\n",
      "2024-09-05 12:54:52 [INFO]: Epoch 004 - training loss: 0.4922, validation loss: 0.3959\n",
      "2024-09-05 12:55:00 [INFO]: Epoch 005 - training loss: 0.4923, validation loss: 0.4118\n",
      "2024-09-05 12:55:09 [INFO]: Epoch 006 - training loss: 0.4835, validation loss: 0.4187\n",
      "2024-09-05 12:55:18 [INFO]: Epoch 007 - training loss: 0.4897, validation loss: 0.4231\n",
      "2024-09-05 12:55:28 [INFO]: Epoch 008 - training loss: 0.4855, validation loss: 0.3985\n",
      "2024-09-05 12:55:37 [INFO]: Epoch 009 - training loss: 0.4604, validation loss: 0.3956\n",
      "2024-09-05 12:55:50 [INFO]: Epoch 010 - training loss: 0.4489, validation loss: 0.4006\n",
      "2024-09-05 12:55:59 [INFO]: Epoch 011 - training loss: 0.4534, validation loss: 0.4268\n",
      "2024-09-05 12:56:08 [INFO]: Epoch 012 - training loss: 0.4678, validation loss: 0.4466\n",
      "2024-09-05 12:56:17 [INFO]: Epoch 013 - training loss: 0.4334, validation loss: 0.4552\n",
      "2024-09-05 12:56:27 [INFO]: Epoch 014 - training loss: 0.4093, validation loss: 0.4275\n",
      "2024-09-05 12:56:27 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2024-09-05 12:56:27 [INFO]: Finished training. The best model is from epoch#9.\n"
     ]
    }
   ],
   "source": [
    "# Fit amd predict\n",
    "raindrop.fit(train_set=datasets[\"training\"], val_set=datasets[\"validation\"])\n",
    "\n",
    "pred_brits = raindrop.predict(datasets[\"testing\"])[\"classification\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing classification metrics: \n",
      "ROC_AUC: 0.7115059221658206, \n",
      "Accuracy: 0.7877551020408163,\n",
      "F1: 0.3157894736842105,\n",
      "Precision: 0.42857142857142855,\n",
      "Recall: 0.25,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate the values of binary classification metrics on the model's prediction\n",
    "metrics = calc_binary_classification_metrics(pred_brits, datasets[\"testing\"][\"y\"])\n",
    "print(\n",
    "    \"Testing classification metrics: \\n\"\n",
    "    f'ROC_AUC: {metrics[\"roc_auc\"]}, \\n'\n",
    "    f'Accuracy: {metrics[\"accuracy\"]},\\n'\n",
    "    f'F1: {metrics[\"f1\"]},\\n'\n",
    "    f'Precision: {metrics[\"precision\"]},\\n'\n",
    "    f'Recall: {metrics[\"recall\"]},\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the algorithm papers below:\n",
    "* [Raindrop: Graph-guided Network for Irregularly Sampled Multivariate Time Series](https://arxiv.org/pdf/2110.05357)\n",
    "* [CSDI: Conditional Score-based Diffusion Model for Probabilistic Timer Series Imputation](https://arxiv.org/pdf/2107.03502)\n",
    "* [BRITS: Bidirectional Recurrent Imputation for Time Series](https://papers.nips.cc/paper_files/paper/2018/file/734e6bfcd358e25ac1db0a4241b95651-Paper.pdf)\n",
    "\n",
    "And tools:\n",
    "* [PyPOTS](https://github.com/WenjieDu/PyPOTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
