{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced missing values imputation techniques\n",
    "In this notebook, we explore some missing value imputation techniques for tabular data on different public datasets. \n",
    "The goal of this workshop is to introduce these different techniques and their implementations in open source libraries.\n",
    "\n",
    "\n",
    "For each dataset, we compare different missing value imputation techniques with a naive baseline in terms of machine learning efficiency: we train a machine learning model that performs a downstream prediction task and we observe the change in performance between the naive baseline and the imputed versions. \n",
    "\n",
    "The main library used in this workshop is **Hyperimpute**, it groups various imputation techniques from naive approaches (e.g. mean, most frequent value) to approcahes based on generative models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install category-encoders\n",
    "!pip install hyperimpute\n",
    "!pip install pypots\n",
    "!pip install autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### IMPUTATION ###\n",
    "from hyperimpute.plugins.imputers import Imputers\n",
    "from pypots.utils.metrics import calc_mae, calc_rmse\n",
    "\n",
    "# ML efficiency\n",
    "from autogluon.tabular import TabularPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Description of types of missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed-type Tabular + Missing Not at Random\n",
    "In this section, we explore imputation for a tabular dataset with mixed-type features (numerical and categorical) that contains missing data not at random. Specifically, in this case, we are looking at the Cirrhosis Patient Survival Prediction dataset. The downstream task we use for machine learning efficiency evaluation is the survival status of the patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load MNAR data\n",
    "cirrhosis = pd.read_csv(\"data/cirrhosis.csv\")\n",
    "categorical_columns = [\n",
    "    \"Status\",\n",
    "    \"Drug\",\n",
    "    \"Sex\",\n",
    "    \"Ascites\",\n",
    "    \"Hepatomegaly\",\n",
    "    \"Spiders\",\n",
    "    \"Edema\",\n",
    "    \"Stage\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a first glance of the data, more specifically at the amount of missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cirrhosis.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to check if the data has the issue of class imbalance to decide which evaluation metric is more appropriate for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cirrhosis.Status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also let's check the class imbalance for rows with any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cirrhosis[cirrhosis.isnull().any(axis=1)].Status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cirrhosis[\n",
    "    cirrhosis.isnull().any(axis=1)\n",
    "].Status.value_counts() / cirrhosis.Status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinal encoding for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder = ce.OrdinalEncoder(handle_missing=\"return_nan\")\n",
    "encoded_df = encoder.fit_transform(cirrhosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    encoded_df, test_size=0.2, stratify=cirrhosis[[\"Status\", \"Sex\"]], random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute machine learning efficiency\n",
    "We use machine learning efficiency to evaluate the performance of the impytation technique. The idea is to compare the performance of a machine learning model on the baseline (without data imputation) against the imputed versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_ml_efficiency(train, test, label, eval_metric=\"f1_macro\"):\n",
    "    \"\"\"\n",
    "    Evaluate machine learning efficiency using AutoGluon's TabularPredictor.\n",
    "\n",
    "    Parameters:\n",
    "    - train:        pd.DataFrame, the training dataset.\n",
    "    - test:         pd.DataFrame, the testing dataset.\n",
    "    - label:        str, the label column.\n",
    "    - eval_metric:  str, the evaluation metric to use.\n",
    "\n",
    "    Returns:\n",
    "    - dict, evaluation results.\n",
    "    \"\"\"\n",
    "    predictor = TabularPredictor(label=label, eval_metric=eval_metric)\n",
    "\n",
    "    # Define the hyperparameters to use only NN_Torch\n",
    "    hyperparameters = {\n",
    "        \"NN_TORCH\": {\n",
    "            \"num_layers\": 3,  # Number of layers in the model\n",
    "            \"hidden_size\": 128,  # Size of the hidden layers\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Fit the model with only NN_Torch\n",
    "    predictor.fit(train, hyperparameters=hyperparameters, presets=\"best_quality\")\n",
    "\n",
    "    return predictor.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline\n",
    "The ML efficiency evaluation on the baseline case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "baseline_res = cal_ml_efficiency(\n",
    "    train=train_df, test=test_df, label=\"Status\", eval_metric=\"f1_macro\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(\n",
    "    algo,\n",
    "    df_missing,\n",
    "    train_indices,\n",
    "    test_indices,\n",
    "    random_state=42,\n",
    "    categorical_cols=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Impute missing values in a DataFrame using the specified algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    - algo:             str, the imputation algorithm to use.\n",
    "    - df_missing:       pd.DataFrame, the original DataFrame with missing values.\n",
    "    - train_indices:    list, the indices of the training set.\n",
    "    - test_indices:     list, the indices of the testing set.\n",
    "    - random_state:     int, the random seed to use.\n",
    "    - categorical_cols: list, the names of the categorical columns.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame, the imputed train and test DataFrames.\n",
    "    \"\"\"\n",
    "    print(\"Available imputing algorithms:\\n\" + \"\\n\".join(Imputers().list()))\n",
    "\n",
    "    imputer = Imputers().get(algo, random_state=random_state)\n",
    "    df_imp = imputer.fit_transform(df_missing.copy())\n",
    "\n",
    "    # assign the column names back\n",
    "    df_imp.columns = df_missing.columns\n",
    "\n",
    "    if categorical_cols:\n",
    "        df_imp[categorical_cols] = df_imp[categorical_cols].round()\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_imp = df_imp.loc[train_indices]\n",
    "    test_imp = df_imp.loc[test_indices]\n",
    "\n",
    "    return train_imp, test_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miss Forest\n",
    "Missing values imputation using Miss Forest: It uses a random forest trained on the observed values of a data matrix to predict the missing values. It can be used to impute continuous and/or categorical data including complex interactions and non-linear relations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_imputed_mf, test_imputed_mf = impute_missing_values(\n",
    "    algo=\"missforest\",\n",
    "    df_missing=encoded_df,\n",
    "    train_indices=train_df.index,\n",
    "    test_indices=test_df.index,\n",
    "    categorical_cols=categorical_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mf_res = cal_ml_efficiency(\n",
    "    train=train_imputed_mf, test=test_imputed_mf, label=\"Status\", eval_metric=\"f1_macro\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAIN\n",
    "GAIN is a missing values imputation method that uses GANs.  The generator observes some components of a real data vector, imputes the missing components conditioned on what is actually observed, and outputs a completed vector. The discriminator then takes a completed vector and attempts to determine which components were actually observed and which were imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_imputed_gain, test_imputed_gain = impute_missing_values(\n",
    "    algo=\"gain\",\n",
    "    df_missing=encoded_df,\n",
    "    train_indices=train_df.index,\n",
    "    test_indices=test_df.index,\n",
    "    categorical_cols=categorical_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gain_res = cal_ml_efficiency(\n",
    "    train=train_imputed_gain,\n",
    "    test=test_imputed_gain,\n",
    "    label=\"Status\",\n",
    "    eval_metric=\"f1_macro\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MIRACLE\n",
    "MIRACLE is causally-aware imputation algorithm. It iteratively refines the imputation of a baseline by simultaneously modeling the missingness generating mechanism, encouraging imputation to be consistent with the causal structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_imputed_miracle, test_imputed_miracle = impute_missing_values(\n",
    "    algo=\"miracle\",\n",
    "    df_missing=encoded_df,\n",
    "    train_indices=train_df.index,\n",
    "    test_indices=test_df.index,\n",
    "    categorical_cols=categorical_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "miracle_res = cal_ml_efficiency(\n",
    "    train=train_imputed_miracle,\n",
    "    test=test_imputed_miracle,\n",
    "    label=\"Status\",\n",
    "    eval_metric=\"f1_macro\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MICE\n",
    "MICE is an algorithm that performs multiple imputations based on regularized linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_imputed_mice, test_imputed_mice = impute_missing_values(\n",
    "    algo=\"mice\",\n",
    "    df_missing=encoded_df,\n",
    "    train_indices=train_df.index,\n",
    "    test_indices=test_df.index,\n",
    "    categorical_cols=categorical_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mice_res = cal_ml_efficiency(\n",
    "    train=train_imputed_mice,\n",
    "    test=test_imputed_mice,\n",
    "    label=\"Status\",\n",
    "    eval_metric=\"f1_macro\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(result, title, y_limit=None, log_scale=False):\n",
    "\t\"\"\"\n",
    "\tPlots a bar chart for comparing evaluation metrics across different methods.\n",
    "\n",
    "\tParameters:\n",
    "\t\"\"\"\n",
    "    # Create a color palette\n",
    "    colors = ['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3', '#a6d854']\n",
    "\n",
    "    # Plotting with a color palette and edges\n",
    "    ax = result.plot(kind=\"bar\", figsize=(12, 7), width=0.8, color=colors[:result.shape[1]], edgecolor='black')\n",
    "\n",
    "    # Adding titles and labels with bigger fonts\n",
    "    plt.title(title, fontsize=16, weight='bold')\n",
    "    plt.xlabel(\"Metrics\", fontsize=12)\n",
    "    plt.ylabel(\"Scores\", fontsize=12)\n",
    "\n",
    "    # Rotating x-axis labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=11)\n",
    "\n",
    "    # Set y-axis limits\n",
    "    if y_limit:\n",
    "        plt.ylim(y_limit)\n",
    "\n",
    "    # Set y-axis to logarithmic scale if log_scale is True\n",
    "    if log_scale:\n",
    "        plt.yscale('log')\n",
    "\n",
    "    # Add grid lines\n",
    "    plt.grid(True, which='major', axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Add a legend with bigger fonts\n",
    "    plt.legend(title=\"Methods\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=11, title_fontsize=12)\n",
    "\n",
    "    # Adjust layout to ensure the plot fits well\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = pd.DataFrame({\n",
    "    \"Baseline\": baseline_res,\n",
    "    \"Miss Forest\": mf_res,\n",
    "    \"GAIN\": gain_res,\n",
    "    \"MIRACLE\": miracle_res,\n",
    "    \"MICE\": mice_res,\n",
    "}).set_index(\"Metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(result=res, title='Classification Metrics', y_limit=(0.8,1), log_scale=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO: COMMENT ON RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Tabular + Missing Completely at Random\n",
    "In this section we explore missing value imputation for a numerical dataset with randomly inserted missing values. For this we use the Wisconson breast cancer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/saurabhbadole/breast-cancer-wisconsin-state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ori = (\n",
    "    pd.read_csv(\n",
    "        \"data/breast-cancer-wisconsin.data\",\n",
    "        delimiter=\",\",\n",
    "        header=None,\n",
    "        names=[\n",
    "            \"Sample code number\",\n",
    "            \"Clump Thickness\",\n",
    "            \"Uniformity of Cell Size\",\n",
    "            \" Uniformity of Cell Shape\",\n",
    "            \"Marginal Adhesion\",\n",
    "            \"Single Epithelial Cell Size\",\n",
    "            \"Bare Nuclei\",\n",
    "            \"Bland Chromatin\",\n",
    "            \"Normal Nucleoli\",\n",
    "            \"Mitoses\",\n",
    "            \"Class\",\n",
    "        ],\n",
    "    )\n",
    "    .drop(columns=[\"Sample code number\"])\n",
    "    .replace(\"?\", np.nan)\n",
    "    .apply(pd.to_numeric, errors=\"coerce\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ori.Class.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate some artificial missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate missing values at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gene_missing_at_random(data, missing_rate, label_col, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate missing data at random in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data:         pd.DataFrame, the input DataFrame.\n",
    "    - missing_rate: float, the proportion of data to be set as missing (0 <= missing_rate <= 1).\n",
    "    - label_col:    str, the name of the label column, which should not be set as missing.\n",
    "    - random_state: int, the random seed to use.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame, the original DataFrame.\n",
    "    - pd.DataFrame, with missing values introduced.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Separate the feature columns and the label column\n",
    "    feature_columns = data.drop(columns=[label_col])\n",
    "    label_column = data[label_col]\n",
    "\n",
    "    # Masking the feature columns with missing values\n",
    "    mask = np.random.rand(*feature_columns.shape) < missing_rate\n",
    "    features_with_missing = feature_columns.mask(mask)\n",
    "\n",
    "    return data, pd.concat([features_with_missing, label_column], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ori, df_miss = gene_missing_at_random(df_ori, 0.3, \"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mar, test_mar = train_test_split(\n",
    "    df_miss, test_size=0.3, stratify=df_miss[[\"Class\"]], random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcar_base_res = cal_ml_efficiency(\n",
    "    train=train_mar,\n",
    "    test=test_mar,\n",
    "    label=\"Class\",\n",
    "    eval_metric=\"f1_macro\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miss Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imputed_mf, test_imputed_mf = impute_missing_values(\n",
    "    algo=\"missforest\",\n",
    "    df_missing=df_miss,\n",
    "    train_indices=train_mar.index,\n",
    "    test_indices=test_mar.index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_res = cal_ml_efficiency(\n",
    "    train=train_imputed_mf,  # .clip(lower=1, upper=10).round()\n",
    "    test=test_imputed_mf,  # .clip(lower=1, upper=10).round()\n",
    "    label=\"Class\",\n",
    "    eval_metric=\"f1_macro\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imputed_gain, test_imputed_gain = impute_missing_values(\n",
    "    algo=\"gain\",\n",
    "    df_missing=df_miss,\n",
    "    train_indices=train_mar.index,\n",
    "    test_indices=test_mar.index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_res = cal_ml_efficiency(\n",
    "    train=train_imputed_gain,  # .clip(lower=1, upper=10).round()\n",
    "    test=test_imputed_gain,  # .clip(lower=1, upper=10).round()\n",
    "    label=\"Class\",\n",
    "    eval_metric=\"f1_macro\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MIRACLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imputed_miracle, test_imputed_miracle = impute_missing_values(\n",
    "    algo=\"miracle\",\n",
    "    df_missing=df_miss,\n",
    "    train_indices=train_mar.index,\n",
    "    test_indices=test_mar.index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miracle_res = cal_ml_efficiency(\n",
    "    train=train_imputed_miracle,  # .clip(lower=1, upper=10).round()\n",
    "    test=test_imputed_miracle,  # .clip(lower=1, upper=10).round()\n",
    "    label=\"Class\",\n",
    "    eval_metric=\"f1_macro\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imputed_mice, test_imputed_mice = impute_missing_values(\n",
    "    algo=\"mice\",\n",
    "    df_missing=df_miss,\n",
    "    train_indices=train_mar.index,\n",
    "    test_indices=test_mar.index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mice_res = cal_ml_efficiency(\n",
    "    train=train_imputed_mice,  # .clip(lower=1, upper=10).round()\n",
    "    test=test_imputed_mice,  # .clip(lower=1, upper=10).round()\n",
    "    label=\"Class\",\n",
    "    eval_metric=\"f1_macro\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {\n",
    "    \"Baseline\": mar_base_res,\n",
    "    \"Miss Forest\": mf_res,\n",
    "    \"GAIN\": gain_res,\n",
    "    \"MIRACLE\": miracle_res,\n",
    "    \"MICE\": mice_res,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics\n",
    "TO DO: Explain what these metrics mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_metrics(original, imputed, metric=\"all\"):\n",
    "    \"\"\"\n",
    "    Calculate imputation quality metrics for given original and imputed datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - original: pd.DataFrame, the original dataset with missing values.\n",
    "    - imputed:  pd.DataFrame, the imputed dataset.\n",
    "    - metric:   str, the metric to calculate ('mae' or 'rmse').\n",
    "\n",
    "    Returns:\n",
    "    - float, the calculated metric value.\n",
    "    \"\"\"\n",
    "    valid_rows = ~original.isnull().any(axis=1)\n",
    "    ori_np = original[valid_rows].to_numpy()\n",
    "    imp_np = imputed[valid_rows].to_numpy()\n",
    "\n",
    "    metrics = {\n",
    "        \"mae\": calc_mae,\n",
    "        \"rmse\": calc_rmse,\n",
    "    }\n",
    "\n",
    "    if metric == \"all\":\n",
    "        return {name: func(ori_np, imp_np) for name, func in metrics.items()}\n",
    "    elif metric in metrics:\n",
    "        return metrics[metric](ori_np, imp_np)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid metric '{metric}'. Choose 'mae', 'rmse', or 'all'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO: PUT THESE RESULTS TOGETHER IN A BARPLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_metrics(\n",
    "    original=df_ori,\n",
    "    imputed=pd.concat([train_imputed_mf, test_imputed_mf])\n",
    "    .sort_index()\n",
    "    .clip(lower=1, upper=10)\n",
    "    .round(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_metrics(\n",
    "    original=df_ori,\n",
    "    imputed=pd.concat([train_imputed_gain, test_imputed_gain])\n",
    "    .sort_index()\n",
    "    .clip(lower=1, upper=10)\n",
    "    .round(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_metrics(\n",
    "    original=df_ori,\n",
    "    imputed=pd.concat([train_imputed_miracle, test_imputed_miracle])\n",
    "    .sort_index()\n",
    "    .clip(lower=1, upper=10)\n",
    "    .round(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_metrics(\n",
    "    original=df_ori,\n",
    "    imputed=pd.concat([train_imputed_mice, test_imputed_mice])\n",
    "    .sort_index()\n",
    "    .clip(lower=1, upper=10)\n",
    "    .round(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_metrics(\n",
    "\toriginal=df_ori,\n",
    "\timputed=pd.concat([train_imputed_miracle, test_imputed_miracle]).sort_index().clip(lower=1, upper=10).round(),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
